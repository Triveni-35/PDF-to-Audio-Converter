{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546cac09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8191c000",
   "metadata": {},
   "source": [
    "Audio Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3ee5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computers in Biology and Medicine 178 (2024) 108726\n",
      "Available online 9 June 2024\n",
      "0010-4825/Â© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n",
      "Contents lists available at ScienceDirect\n",
      "Computers in Biology and Medicine\n",
      "journal homepage: www.elsevier.com/locate/compbiomed\n",
      "HTC-retina: A hybrid retinal diseases classification model using\n",
      "transformer-Convolutional Neural Network from optical coherence\n",
      "tomography images\n",
      "Ayoub Laouarema,âˆ—, Chafia Kara-Mohameda, El-Bay Bourennaneb, Aboubekeur Hamdi-Cherifa\n",
      "aDepartment of Computer Science, University of Ferhat Abbas 1, Setif, Algeria\n",
      "bImViA Laboratory, University of Burgundy, Dijon, France\n",
      "A R T I C L E I N F O\n",
      "Keywords:\n",
      "Retinal disease\n",
      "Optical coherence tomography\n",
      "Deep learning\n",
      "Convolutional Neural Network\n",
      "Vision transformer\n",
      "HybridizationA B S T R A C T\n",
      "Retinal diseases are among nowadays major public health issues, deservedly needing advanced computer-\n",
      "aided diagnosis. We propose a hybrid model for multi label classification, whereby seven retinal diseases are\n",
      "automatically classified from Optical Coherence Tomography (OCT) images. We show that, by combining the\n",
      "strengths of Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs), we can produce a more\n",
      "powerful type of model for medical image classification, especially when considering local lesion information\n",
      "such as retinal diseases. CNNs are indeed proved to be efficient at parameter utilization and provide the ability\n",
      "to extract local features and multi-scale feature maps through convolutional operations. On the other hand,\n",
      "ViTâ€™s self-attention procedure allows processing long-range and global dependencies within an image. The\n",
      "paper clearly shows that the hybridization of these complementary capabilities (CNNs-ViTs) presents a high\n",
      "image processing potential that is more robust and efficient.\n",
      "The proposed model adopts a hierarchical CNN module called Convolutional Patch and Token Embedding\n",
      "(CPTE) instead of employing a direct tokenization approach using the raw input OCT image in the transformer.\n",
      "The CPTE moduleâ€™s role is to incorporate an inductive bias, to reduce the reliance on large-scale datasets, and to\n",
      "address the low-level feature extraction challenges of the ViT. In addition, considering the importance of local\n",
      "lesion information in OCT images, the model relies on a parallel module called Residual Depthwise-Pointwise\n",
      "ConvNet (RDP-ConvNet) for extracting high-level features. RDP-ConvNet utilizes depthwise and pointwise\n",
      "convolution layers within a residual network architecture. The overall performance of the HTC-Retina model\n",
      "was evaluated on three datasets: the OCT-2017, OCT-C8, and OCT-2014 ; outperforming previous established\n",
      "models, achieving accuracy rates of 99.40%, 97.00%, and 99.77%, respectively ; and sensitivity rates of\n",
      "99.41%, 97.00%, and 99.77%, respectively. Notably, the model showed high performance while maintaining\n",
      "computational efficiency.\n",
      "1. Introduction\n",
      "The prevalence of retinal diseases has witnessed a notable surge in\n",
      "recent years, causing significant visual impairments and abnormalities.\n",
      "These conditions encompass a range of disorders that inflict damage\n",
      "upon various components of the retina, consequently leading to tem-\n",
      "porary or permanent vision loss and visual abnormalities [ 1]. Among\n",
      "the most prevalent retinal diseases cases, seven types are treated in\n",
      "this study: Age-related Macular Degeneration (AMD) [ 2], Diabetic Mac-\n",
      "ular Edema (DME) [ 3], DRUSEN [ 4], Choroidal Neo-Vascularization\n",
      "(CNV) [ 5], Central Serous Retinopathy (CSR) [ 6], Diabetic Retinopathy\n",
      "(DR) and Macular Hole (MH).\n",
      "Optical Coherence Tomography (OCT) represents a pivotal imaging\n",
      "technique for the examination of retinal diseases. It is characterized\n",
      "âˆ—Corresponding author.\n",
      "E-mail address: laouarem.ayoub@univ-setif.dz (A. Laouarem).by its non-contact, non-invasive nature and high-speed imaging capa-\n",
      "bilities, allowing for the visualization of cross-sectional layers within\n",
      "the retina [ 7]. The OCT technology holds significant importance in\n",
      "the field of ophthalmology, facilitating the quantification, analysis,\n",
      "and treatment planning for various retinal disorders. By enabling the\n",
      "detection of early-stage pathologies before the manifestation of visual\n",
      "symptoms and irreversible vision loss, OCT plays a crucial role in\n",
      "identifying treatable conditions at suitable time points. Fig. 1 depicts\n",
      "examples of OCT scans from eight categories of retina diseases.\n",
      "With the advancement of biomedical imaging and sensing tech-\n",
      "niques, as well as the development of high-performance computers and\n",
      "machine learning algorithms, intelligent disease diagnosis has become\n",
      "https://doi.org/10.1016/j.compbiomed.2024.108726\n",
      "Received 18 November 2023; Received in revised form 4 June 2024; Accepted 7 June 2024 Computers in Biology and Medicine 178 (2024) 108726\n",
      "2A. Laouarem et al.\n",
      "Fig. 1. Examples of OCT images in eight classes . (a) Normal, (b) CNV, (c) Drusen,\n",
      "(d) DME, (e) CSR, (f) AMD, (g) DR, and (h) MH.\n",
      "a reality. CNNs have particularly shown remarkable success in medical\n",
      "image classification by effectively extracting local features from the\n",
      "entire image and providing well-informed inductive biases. Numer-\n",
      "ous researchers have achieved significant breakthroughs in the OCT\n",
      "image classification for disease diagnosis using CNNs. However, in\n",
      "the analysis of retinal diseases, relying on more local lesion features\n",
      "may be required to achieve optimal performance. Therefore, there is a\n",
      "need to incorporate global retinal information along with local lesion\n",
      "information to enhance classification performance.\n",
      "On the other hand, the emergence of vision transformers (ViTs) as\n",
      "the prevailing architecture in computer vision has opened new possi-\n",
      "bilities. Originally designed for sequence modeling and transduction\n",
      "tasks, ViT models leverage self-attention mechanisms to capture long-\n",
      "range dependencies among image pixels, and they have demonstrated\n",
      "promising results in image classification tasks. However, employing\n",
      "a transformer architecture alone may pose challenges in achieving\n",
      "satisfactory performance on small-scale datasets. Indeed, transformers\n",
      "lack the local inductive bias and typically require large amounts of\n",
      "training data, inducing a significant challenge in the context of retinal\n",
      "OCT image classification, where the availability of training images is\n",
      "relatively limited.\n",
      "As a potential result, combining the strengths of CNNs and ViT\n",
      "might produce more effective models for medical image analysis tasks\n",
      "especially when dealing with local lesion information. It is indeed\n",
      "demonstrated that CNNs offer the advantages of efficient parameter uti-\n",
      "lization and the ability to extract local features and multi-scale feature\n",
      "maps through convolutional operations. Meanwhile, ViTâ€™s self-attention\n",
      "mechanism allows for processing long-range and global dependencies\n",
      "within an image. The integration of these complementary capabilities\n",
      "of CNNs and ViT has the potential to enable the model to process image\n",
      "information more robustly and efficiently. In recent studies [ 8â€“10],\n",
      "researchers have explored the combination of CNNs and transformers\n",
      "in image analysis tasks. The objective was to leverage the respective\n",
      "strengths of each algorithm and create a hybrid model capable of\n",
      "achieving superior performance across various data sizes.\n",
      "In this study, we propose a hybrid architecture called HTC-Retina\n",
      "for classifying retinal diseases using OCT images. The HTC-Retina\n",
      "model combines the strengths of transformers in capturing long-range\n",
      "dependencies and CNNs in extracting hierarchical local features. Specif-\n",
      "ically, instead of employing a direct tokenization approach using the\n",
      "raw input OCT image in the transformer, we adopt a hierarchical\n",
      "convolutional network module called Convolutional Patch and To-\n",
      "ken Embedding (CPTE). This module incorporates an inductive bias,\n",
      "reduces the reliance on large-scale datasets, and addresses the low-\n",
      "level feature extraction challenges of the ViT. Furthermore, considering\n",
      "the importance of local lesion information in OCT images, we em-\n",
      "ploy a parallel module called Residual Depthwise-Pointwise ConvNet\n",
      "(RDP-ConvNet) for extracting high-level features. RDP-ConvNet utilizes\n",
      "depthwise and pointwise convolution layers within a residual net-\n",
      "work architecture. Many lightweight deep learning modules, includingLPRNet [ 11] and LRPRNet [ 12], combine depthwise and pointwise\n",
      "convolutions within a residual network architecture. To assess the\n",
      "effectiveness and generality of the proposed HTC-Retina model, we\n",
      "conduct experiments on three publicly available OCT datasets, namely\n",
      "OCT-2017 , OCT-C8 , and OCT-2014 datasets.\n",
      "In summary, the contributions of this work are as follows:\n",
      "â€¢We propose a hybrid ViT-CNN model, called HTC-Retina, specif-\n",
      "ically designed for classifying retinal diseases using OCT images.\n",
      "The model incorporates the CPTE module and RDP-ConvNet mod-\n",
      "ule. By combining these modules with the Vision Transformer\n",
      "Encoder (ViTE) module, our model effectively captures both local\n",
      "features and long-range dependency information without relying\n",
      "on position embeddings.\n",
      "â€¢We use other transfer learning models and investigate the feasibil-\n",
      "ity of the ViT model for retinal OCT image classification to com-\n",
      "pare its performances with the proposed HTC-Retina model. This\n",
      "analysis provides insights into the effectiveness of our approach\n",
      "compared to established methodologies.\n",
      "â€¢We conduct experiments on three distinct datasets, each com-\n",
      "prising different types of retinal diseases. This comprehensive\n",
      "evaluation allows us to assess the performance of our proposed\n",
      "HTC-Retina model across a diverse range of retinal conditions.\n",
      "â€“We experimentally verify that our proposed HTC-Retina\n",
      "model outperforms the baseline, transfer learning, and some\n",
      "state-of-the-art (SOTA) models and demonstrate the gener-\n",
      "ality of our model with the external validation process.\n",
      "â€“We generate a heatmap based on the Grad-CAM to gain a\n",
      "deeper understanding of the modelâ€™s decision-making pro-\n",
      "cess. By overlaying these heat maps on the original images,\n",
      "we can identify the regions of interest that influence the\n",
      "modelâ€™s predictions.\n",
      "â€“We evaluate the proposed model performance on noisy\n",
      "datasets, to assess the modelâ€™s robustness. The results\n",
      "demonstrate that the HTC-Retina model exhibits robust-\n",
      "ness to moderate levels of noise, highlighting its ability to\n",
      "maintain accurate classification performance even in the\n",
      "presence of noise.\n",
      "â€¢For the purpose of confirming the viability of the HTC-Retina\n",
      "model, four ablation studies are carried out. The aim of this\n",
      "investigation is to find out how each module affected the modelâ€™s\n",
      "overall performance; specifically how the CPTE and RDP-ConvNet\n",
      "modules impacted the functionality of HTC-Retina in OCT scans.\n",
      "Additionally, the impact of eliminating positional embeddings\n",
      "from the model is also investigated.\n",
      "The article is organized as follows: A brief overview and the most\n",
      "recent related works of OCT image classification are presented in\n",
      "Section 2, while the details of the proposed HTC-Retina model are pre-\n",
      "sented in Section 3. In Section 4, the training details are described and\n",
      "comprehensive experimental results and comparisons are discussed.\n",
      "Finally, the article is concluded in Section 5.\n",
      "2. Related works\n",
      "Deep learning (DL) techniques have gained widespread use in var-\n",
      "ious areas of medical imaging, including computer-aided diagnosis,\n",
      "image segmentation, and image-guided therapy. These applications\n",
      "demonstrate the potential for machine learning to improve healthcare\n",
      "outcomes for patients. In particular, several studies have focused on the\n",
      "use of deep learning, especially CNN for OCT imaging. Rong et al. [ 13]\n",
      "proposed the use of a surrogate-assisted CNN with image denoising,\n",
      "thresholding, and morphological dilation for mask extraction to gen-\n",
      "erate surrogate images for CNN training. This approach achieved an\n",
      "area under the receiver operating characteristics (AUROC) curve values Computers in Biology and Medicine 178 (2024) 108726\n",
      "3A. Laouarem et al.\n",
      "of 97.8% and 98.6% for AMD detection in internal validation and\n",
      "external testing datasets, respectively. Treder et al. [14] investigated\n",
      "the use of a DL model, pre-trained on the Inception-v3 network [15],\n",
      "to detect AMD in cross-sectional OCT B-scans. The model demonstrated\n",
      "a sensitivity of 100%, specificity of 92.0%, and accuracy of 96.0%.\n",
      "Li et al. [16] improved upon previous work by fine-tuning a VGG-\n",
      "16 network [17], resulting in slightly better performance with an\n",
      "AUROC value of 1, a sensitivity of 97.8%, a specificity of 99.4%,\n",
      "and an accuracy of 98.6%. In a paper by Shenghua He et al. [18], a\n",
      "deep CNN was proposed for the classification task. The CNN architec-\n",
      "ture consisted of nine convolutional blocks and two fully-connected\n",
      "layers. The model was trained and evaluated on a dataset of 269\n",
      "OCT images and achieved an average prediction accuracy of 86.6%.\n",
      "However, the test set accuracy was not reported in the paper. Kermany\n",
      "et al. [19] demonstrated that deep learning-based analysis of OCT B-\n",
      "scans could accurately differentiate images with CNV and DME from\n",
      "images with DRUSEN and NORMAL tissue. The model, pre-trained on\n",
      "the Inception-v3 network [15], achieved an AUROC value of 99.9%,\n",
      "a sensitivity of 97.8%, a specificity of 97.4%, and an accuracy of\n",
      "96.6%. In addition, the study found that a limited model trained with\n",
      "only 1,000 cross-sectional OCT images performed similarly to a model\n",
      "trained on a larger dataset of 108,312 images (AUROC 0.988, sensi-\n",
      "tivity 96.6%, specificity 94.0%, and accuracy 93.4%). Importantly, the\n",
      "researchers made the OCT dataset with ground truth labels available\n",
      "for further research. Karthik and Mahadevappa et al. [20] introduced\n",
      "a contemporary diagnostic system for the classification of OCT im-\n",
      "ages encompassing eight distinct diseases. In their approach, they\n",
      "substitute the residual connection within three ResNet architectures\n",
      "with the EdgeEn block and cross-activation mechanism to enhance the\n",
      "contrast of derivatives and generate more pronounced features. This\n",
      "modification effectively contributes to improved classification accu-\n",
      "racy. Notably, the ResNet50 model achieves the most favorable results,\n",
      "exhibiting an accuracy of 90.30%, a recall of 91.00%, a precision of\n",
      "91.00%, and an F1-score of 91.00%. F ENG L I et al. [21] proposed\n",
      "an algorithm for classifying retinal diseases in OCT images. The al-\n",
      "gorithm uses an ensemble of four improved residual neural network\n",
      "(ResNet50) [22] models to classify OCT images into four classes: CNV,\n",
      "DME, DRUSEN, and NORMAL. The algorithm is evaluated using a\n",
      "patient-level 10-fold cross-validation process on a development dataset,\n",
      "and it is shown to achieve accuracy of 97.3%, sensitivity of 96.3% and\n",
      "specificity of 98.5%. The paper also discusses using occlusion testing\n",
      "and integrating medical history data to improve model performance.\n",
      "In a paper by Gokhan Altan [23], a lightweight explainable CNN archi-\n",
      "tecture, called DeepOCT, was proposed for the identification of macular\n",
      "edema on OCT images. The proposed model focused on achieving\n",
      "high classification performance while utilizing pre-trained architectures\n",
      "and shallow dense layers. DeepOCT also aimed to visualize feature\n",
      "activation mapsâ€™ most relevant regions and pathologies. To achieve this,\n",
      "the model incorporated the block-matching and 3D filtering algorithm,\n",
      "flattened the retinal layers to mitigate the effects of different macula\n",
      "positions, and excluded non-retinal layers through cropping. DeepOCT\n",
      "achieved accuracy, sensitivity, and specificity rates of 99.20%, 100%,\n",
      "and 98.40%, respectively, for the identification of OCT images with\n",
      "macular edema. Berrimi et al. [24] proposed a deep CNN architecture,\n",
      "consisting of three convolutional layers with dropout and batch normal-\n",
      "ization. The proposed model achieved an accuracy of 98.65%, a recall\n",
      "of 98.50%, a precision of 98.75%, and an F1-score of 98.50%.\n",
      "On the other hand, transformers, which were initially developed for\n",
      "Natural Language Processing tasks [25], have recently been extended to\n",
      "computer vision using deep-learning approaches. Dosovitskiy et al. [26]\n",
      "introduced the Vision Transformer (ViT) architecture. This architecture\n",
      "has shown promising results in image classification tasks, leading to the\n",
      "development of various transformer-based models like the Swin Trans-\n",
      "former [27], Twins [28], and CrossViT [29], serving as general-purpose\n",
      "backbones for computer vision. Despite the success of the ViT archi-\n",
      "tecture in medical imaging tasks such as breast and brain cancers, aswell as various diseases, it is noted that transformers require substantial\n",
      "training data for optimal performance [30]. Hence, recent research has\n",
      "employed hybrid ViT-CNN architectures, exemplified by models such\n",
      "as CoAtNet proposed by Dai et al. [31]. This architecture is grounded\n",
      "in the integration of depthwise convolution and self-attention, achieved\n",
      "through the stacking of convolutional layers and attention layers. Jun-\n",
      "yong Shen et al. [32] proposed a Structure-Oriented Transformer (SoT)\n",
      "framework for retinal disease grading, utilizing a self-attention mech-\n",
      "anism to capture the relationship between the lesion and the retina.\n",
      "The model outperformed SOTA methods in grading neovascular AMD\n",
      "and was evaluated on a publicly available retinal diseases dataset,\n",
      "demonstrating its classification superiority and good generality. A study\n",
      "by [33] introduced a Hybrid ConvNet-Transformer network (HCT-\n",
      "Net). The HCTNet incorporates a low-level feature extraction module\n",
      "based on the residual dense block for initial low-level feature gener-\n",
      "ation. Two parallel branches, utilizing Transformer and Convolution\n",
      "architectures, are then employed to capture global and local context\n",
      "within OCT images. A feature fusion block, employing an adaptive\n",
      "re-weighting mechanism, combines the global and local features. The\n",
      "model achieved superior performance, surpassing pure ViT and several\n",
      "ConvNet-based classification methods. Verification on two public reti-\n",
      "nal OCT datasets demonstrated HCTNetâ€™s overall accuracies of 91.56%\n",
      "and 86.18%, respectively. P. Dutta et al. [34] proposed Conv-ViT, a\n",
      "hybrid ViT-CNN model. Conv-ViT leverages the capabilities of transfer\n",
      "learning-based CNN models, specifically Inception-V3 and ResNet-50,\n",
      "for extracting texture information, alongside a vision transformer for\n",
      "capturing shape-based features. This hybridization facilitates the robust\n",
      "learning of shape-based texture features, achieving an accuracy of\n",
      "approximately 94% in the classification of retinal diseases into four\n",
      "distinct classes. In a distinct study conducted by Huajie Wen et al. [35],\n",
      "a hybrid model called Lesion-Localization Convolution Transformer\n",
      "(LLCT) was employed to enhance the classification accuracy, sensitiv-\n",
      "ity, and specificity of OCT images in ophthalmic disease diagnosis. This\n",
      "architecture integrates convolution and self-attention, utilizing custom\n",
      "feature maps from a CNN as input sequences for the self-attention\n",
      "network. The model attains notable performance metrics, achieving\n",
      "approximately 97.7% overall accuracy, 97.7% sensitivity, and 99.2%\n",
      "specificity.\n",
      "3. Methods\n",
      "3.1. Model structure\n",
      "Vision transformers (ViT) [26] are known for their global self-\n",
      "attention mechanisms [25], making them effective for capturing long-\n",
      "range dependencies but less adept at local feature extraction. In retinal\n",
      "disease classification, ViT may struggle with irrelevant information and\n",
      "obscure important features [30]. To address this, our study proposes\n",
      "HTC-Retina, a hybrid ViT-CNN model. The overall architectural rep-\n",
      "resentation is illustrated in Fig. 2. The HTC-Retina model integrates a\n",
      "Convolutional Patch and Token Embedding (CPTE) 3.2 for low-level\n",
      "local dependencies and patterns essential for the formation of over-\n",
      "lapping tokens and patches. Simultaneously, the sequence of patches\n",
      "and tokens undergoes processing in two parallel modules: the ViTE\n",
      "3.3 module and the RDP-ConvNet module 3.4. This parallel processing\n",
      "scheme aims to encode both global and high-level local features of the\n",
      "retinal image, while also modeling the intricate relationship between\n",
      "specific lesion regions and the entire retinal layer. The ViTE module\n",
      "includes self-attention, while the RDP-ConvNet module employs a fully-\n",
      "convolutional block. The outputs from both modules undergo global\n",
      "average pooling and feature fusion combine the extracted features,\n",
      "followed by a fully-connected layer for classification. Computers in Biology and Medicine 178 (2024) 108726\n",
      "4A. Laouarem et al.\n",
      "Fig. 2. The overall architecture of the HTC-Retina model. First, the inputs are fed to a Convolutional Patch and Token Embedding (CPTE) to obtain patches and tokens. Second,\n",
      "the sequence of patches and tokens is subjected to parallel processing in two modules, namely ViTE and RPD-ConvNet, each composed of multiple blocks. Following this, a global\n",
      "average pooling block is applied to the output of each module, followed by dropout regularization. Lastly, a fusion module is employed to effectively concatenate the extracted\n",
      "global and local features, which are then fed into a softmax fully-connected layer to generate classification predictions.\n",
      "Fig. 3. Detailed architecture of the CPTE module .\n",
      "3.2. Convolutional patch and token embedding module\n",
      "The CPTE module is a simple CNN that extracts low-level local\n",
      "features from the input image, generating patches and overlapping\n",
      "tokens through a convolutional block. This block follows a standard\n",
      "architecture involving a convolution, ReLU activation [ 36], and max-\n",
      "pooling. For an image or feature map ğ‘¥âˆˆğ‘…ğ‘ŠÃ—ğ»Ã—ğ¶, the operation is\n",
      "represented as:\n",
      "ğ‘‹=ğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™ (ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ¶ğ‘‚ğ‘ğ‘‰ (ğ‘¥))) (1)\n",
      "Here, the convolution operation (CONV) employs d filters, matching\n",
      "the embedding dimension of the ViTE and the RDP-ConvNet backbone.\n",
      "The CPTE module comprises three hierarchical convolutional layers\n",
      "with 64, 128, and 256 filters, respectively, as depicted in Fig. 3. Each\n",
      "layer uses 3 Ã—3 filters, with a subsequent zero-padding followed by\n",
      "a max-pooling layer. In summary, the CPTE module produces spatially\n",
      "localized patches used by the RDP-ConvNet module, alongside generat-\n",
      "ing a sequential arrangement of tokens by reshaping the patches. These\n",
      "representations serve as inputs for the subsequent processing by the\n",
      "ViTE module.\n",
      "3.3. Vision transformer encoder-based global dependencies extraction mod-\n",
      "ule\n",
      "The ViTE module comprises a sequence of four identical encoders,\n",
      "as illustrated in Fig. 4. Each encoder unit is composed of a normal-\n",
      "ization layer (NL) [ 37], a multi-head self-attention (MSA) layer, a\n",
      "multi-layer perceptron (MLP), and a stochastic depth regularization\n",
      "(SDR) mechanism [ 38].\n",
      "The MSA layer involves three linear transformations applied to\n",
      "input embeddings, creating query ğ‘„ğ‘–, keyğ¾ğ‘–, and value ğ‘‰ğ‘–vectors\n",
      "for each embedding vector ğ‘‹ğ‘–. These vectors are used to compute the\n",
      "similarity between tokens, and the resulting attention matrix enhances\n",
      "tokens by multiplying with corresponding value vectors. This process\n",
      "is performed for multiple attention heads, denoted as â„, and the outputtokens are concatenated. The mathematical representation is provided\n",
      "as follows:\n",
      "â„ğ‘’ğ‘ğ‘‘ğ‘–=ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘„,ğ¾,ğ‘‰ ) (2)\n",
      "ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘„,ğ¾,ğ‘‰ ) =ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘‹ğ‘Šğ‘„(ğ‘‹ğ‘Šğ¾)ğ‘‡\n",
      "âˆš\n",
      "ğ‘‘ğ‘˜)ğ‘‹ğ‘Šğ‘‰(3)\n",
      "ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ =ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (â„ğ‘’ğ‘ğ‘‘1,â„ğ‘’ğ‘ğ‘‘2,â€¦,â„ğ‘’ğ‘ğ‘‘â„)ğ‘Šğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ (4)\n",
      "The subsequent MLP layer, activated by the Gaussian Error Linear\n",
      "Unit (GELU) [ 39], incorporates non-linear transformations. Both the\n",
      "MSA and MLP layers feature skip connections, similar to residual\n",
      "networks, and undergo layer normalization. Stochastic depth regu-\n",
      "larization [ 38] is applied during training, which is a technique that\n",
      "randomly drops a set of layers, whereas, during inference, all layers\n",
      "are retained. The equations governing this process are:\n",
      "Ì‚ğ‘‹ğ‘™=ğ‘†ğ·ğ‘… (ğ‘€ğ‘†ğ´ (ğ¿ğ‘(ğ‘‹ğ‘™âˆ’1))) +ğ‘‹ğ‘™âˆ’1 (5)\n",
      "Ì‚ğ‘‹ğ‘™=ğ‘†ğ·ğ‘… (ğ‘€ğ¿ğ‘ƒ (ğ¿ğ‘(Ì‚ğ‘‹ğ‘™))) +Ì‚ğ‘‹ğ‘™ (6)\n",
      "Finally, a residual connection [ 40] is added between the output\n",
      "of the last layer and the original input embeddings at the end of the\n",
      "encoder, which helps to preserve the information.\n",
      "In summary, the ViTE module efficiently processes input sequences\n",
      "using self-attention to model token dependencies and feedforward net-\n",
      "works for introducing non-linearities. Residual connections and layer\n",
      "normalization are employed to stabilize training and mitigate the van-\n",
      "ishing gradient problem.\n",
      "3.4. Residual depthwise-pointwise ConvNet-based local dependencies ex-\n",
      "traction module\n",
      "While the ViTE excels at collecting global and long-range contex-\n",
      "tual information, it ignores local features and the underlying two-\n",
      "dimensional neighborhood structure in pictures. Based on recent ad-\n",
      "vances in the area, a parallel Residual Depthwise-Pointwise ConvNet Computers in Biology and Medicine 178 (2024) 108726\n",
      "5A. Laouarem et al.\n",
      "Fig. 4. Detailed architecture of the ViTE module .\n",
      "Fig. 5. Detailed architecture of the RDP-ConvNet module .\n",
      "(RDP-ConvNet) module is designed to capture high-level local fea-\n",
      "tures [ 41,42]. The RDP-ConvNet module as illustrated in Fig. 5 is\n",
      "structured as a fully convolutional block, employing two key convo-\n",
      "lutional operations: depthwise convolution with a subsequent residual\n",
      "connection, followed by pointwise convolution. In depthwise convo-\n",
      "lution, grouped convolution is performed with the number of groups\n",
      "equating to the number of channels. Subsequently, pointwise convolu-\n",
      "tion, characterized by a kernel size of 1, is applied. Each convolution\n",
      "operation is succeeded by a GELU activation function layer [ 39].\n",
      "Represented by ğ‘‹ğ‘™âˆ’1as the previous layerâ€™s output and ğœas the GELU\n",
      "non-linear activation, the output of the current layer is expressed as:\n",
      "Ì‚ğ‘‹ğ‘™=ğœ(ğ¶ğ‘œğ‘›ğ‘£ğ·ğ‘’ğ‘ğ‘¡â„ğ‘¤ğ‘–ğ‘ ğ‘’ (Ì‚ğ‘‹ğ‘™âˆ’1)) +Ì‚ğ‘‹ğ‘™âˆ’1 (7)\n",
      "Ì‚ğ‘‹ğ‘™+1=ğœ(ğ¶ğ‘œğ‘›ğ‘£ğ·ğ‘’ğ‘ğ‘¡â„ğ‘¤ğ‘–ğ‘ ğ‘’ (Ì‚ğ‘‹ğ‘™)) (8)\n",
      "3.5. Fusion module\n",
      "The global features from the ViTE module and the high-level lo-\n",
      "cal features from the RDP-ConvNet undergo global average pooling,\n",
      "transforming the output features into a 1-D representation. Subse-\n",
      "quently, a concatenation approach is employed to fuse the outputs,\n",
      "creating a combined feature representation that integrates both global\n",
      "and local information. Denoting ğ‘‚ğ‘‰ğ‘–ğ‘‡ğ¸,ğ‘‚ğ‘…ğ·ğ‘ƒğ¶ğ‘œğ‘›ğ‘£ğ‘ğ‘’ğ‘¡ as the ViTE and\n",
      "RDP-ConvNet outputs respectively, and ğºğ´ğ‘ƒ as the Global Average\n",
      "Pooling operation, the final output ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘“of the fusion module can\n",
      "be succinctly expressed as:\n",
      "ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘“=ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (ğºğ´ğ‘ƒ1ğ·(ğ‘‚ğ‘‰ğ‘–ğ‘‡ğ¸ ),ğºğ´ğ‘ƒ2ğ·(ğ‘‚ğ‘…ğ·ğ‘ƒğ¶ğ‘œğ‘›ğ‘£ğ‘ğ‘’ğ‘¡ )) (9)\n",
      "3.6. Classification module\n",
      "The concatenated output is fed through a fully connected neural\n",
      "network consisting of a single layer with four neurons and a softmax ac-\n",
      "tivation function. This facilitates class assignment based on the highest\n",
      "probability value in the resulting distribution.3.7. Loss function\n",
      "During training, the HTC-Retina model is trained using cross-entropy\n",
      "loss [ 42]. The loss function, denoted as ğ¿, is defined as follows:\n",
      "ğ¿= âˆ’1\n",
      "ğ‘ğ‘âˆ’1âˆ‘\n",
      "ğ‘–=0ğ¾âˆ’1âˆ‘\n",
      "ğ‘—=0ğ‘¦ğ‘—\n",
      "ğ‘–ğ‘™ğ‘œğ‘”(ğ‘ğ‘—\n",
      "ğ‘–) (10)\n",
      "whereğ‘is the number of samples, ğ¾is the number of categories. ğ‘ğ‘—\n",
      "ğ‘–\n",
      "represents the ğ‘—thprobability of the ğ‘–thsample, and ğ‘¦ğ‘—\n",
      "ğ‘–represents the\n",
      "ğ‘—thlabel of the ğ‘–thsample.\n",
      "4. Experiments and results\n",
      "4.1. Datasets and preprocessing\n",
      "4.1.1. Datasets\n",
      "Deep learning models have demonstrated exemplary performance\n",
      "on large-scale datasets [ 43]. However, acquiring and annotating such\n",
      "datasets for medical imaging is challenging due to several factors. One\n",
      "major obstacle is obtaining access to large and diverse datasets that\n",
      "represent the target population accurately. High-quality annotations for\n",
      "medical images also require specialized expertise, time, and resources\n",
      "for quality control and validation. Ethical concerns, such as patient\n",
      "privacy and informed consent, must also be addressed adequately.\n",
      "Consequently, we opted to use three existing OCT image datasets.\n",
      "â€¢OCT-2017 dataset (Balanced version)1contains 48,574 images\n",
      "in JPEG format divided into four categories: NORMAL, CNV,\n",
      "DME, and DRUSEN. The dataset is organized into training, test-\n",
      "ing, and validation folders containing subfolders for each image\n",
      "category.\n",
      "1OCT-2017 Dataset: http://data.mendeley.com/datasets/rscbjbr9sj/2 Computers in Biology and Medicine 178 (2024) 108726\n",
      "6A. Laouarem et al.\n",
      "Table 1\n",
      "Datasetsâ€™ information and distribution.\n",
      "Dataset Classes Training Validation Testing Total\n",
      "OCT-2017 4 38,552 9036 986 48,574\n",
      "OCT-2014 3 2141 645 445 3231\n",
      "OCT-C8 8 18,400 2800 2800 24,000\n",
      "â€¢OCT-2014 dataset (Srinivasan version)2which consists of SD-\n",
      "OCT volumetric scans acquired from 45 patients with three kinds\n",
      "of retinal diseases: 15 normal patients, 15 patients with Early\n",
      "AMD, and 15 patients with Diabetic Macular Edema (DME). It\n",
      "contains 3231 images in JPG format divided into three categories:\n",
      "NORMAL, AMD, and DME.\n",
      "â€¢OCT-C8 dataset (2021 version)3contains 24,000 images in JPG\n",
      "format divided into eight categories: AMD, CNV, CSR, DME, DR,\n",
      "DRUSEN, MH, and NORMAL. The dataset is also organized into\n",
      "training, testing, and validation folders containing subfolders for\n",
      "each image category.\n",
      "Table 1 shows details of the three datasets.\n",
      "4.1.2. Preprocessing and augmentations\n",
      "All images were resized to 224 Ã—224 pixels and normalized using\n",
      "maxâ€“min normalization as follows:\n",
      "ğ‘‹ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘’ğ‘‘ = (ğ‘‹âˆ’ğ‘‹ğ‘šğ‘–ğ‘›)ğ‘‹ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘’ğ‘‘ âˆ’ğ‘šğ‘–ğ‘› â‹…ğ‘‹ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘’ğ‘‘ âˆ’ğ‘šğ‘ğ‘¥\n",
      "ğ‘‹ğ‘šğ‘ğ‘¥âˆ’ğ‘‹ğ‘šğ‘–ğ‘›(11)\n",
      "Whereğ‘‹refers to the original intensity of the input image, ğ‘‹ğ‘šğ‘–ğ‘›and\n",
      "ğ‘‹ğ‘šğ‘ğ‘¥refers to the minimum and maximum pixel values in the original\n",
      "image,ğ‘‹ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘’ğ‘‘ âˆ’ğ‘šğ‘–ğ‘›andğ‘‹ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘’ğ‘‘ âˆ’ğ‘šğ‘ğ‘¥are the required minimum and\n",
      "maximum values for the normalized image, respectively.\n",
      "Data augmentation techniques: RandomGridShuffle, RandomCrop,\n",
      "RandomGamma, Rescaling, and HorizontalFlip were used to improve\n",
      "the dataset and enhance the modelâ€™s performance. The augmentation\n",
      "techniques applied to the input OCT scans were chosen thoughtfully\n",
      "and on purpose to avoid the random selection of parameters based on\n",
      "subjective intuition. Such uninformed parameter choices can result in\n",
      "undesirable transformations being applied to the OCT scans, leading to\n",
      "feature deformation and eventual loss.\n",
      "To address this concern, we methodically controlled the augmen-\n",
      "tation process, especially the RandomGridShuffle and RandomCrop\n",
      "techniques, to preserve the Region of Interest (RoI) within the images.\n",
      "This ensures that disease-specific features, crucial for accurate classifi-\n",
      "cation, remain unaffected, thereby maintaining the diagnostic accuracy\n",
      "of the output images. By concentrating transformations on non-critical\n",
      "regions while preserving the RoI, we undertake a reasonable trade-\n",
      "off between introducing variability and retaining meaningful image\n",
      "content.\n",
      "The aforementioned techniques are used because they offer several\n",
      "benefits, such as increased spatial robustness, improved handling of\n",
      "image variations, adaptability to different lighting conditions, standard-\n",
      "ization of image sizes, and orientation invariance. These advantages\n",
      "contribute to the enhanced performance and generalization capabilities\n",
      "of the model in the task of OCT image classification.\n",
      "4.2. Implementation details and hyperparameters\n",
      "Our model was built using the TensorFlow framework [44] and\n",
      "the experiments were conducted using an NVIDIA Tesla P100 GPU.\n",
      "The model was trained to optimize the cross-entropy loss. AdamW\n",
      "(Stochastic Gradient Descent) [45] was used as the optimizer, with\n",
      "2OCT-2014 Dataset: http://people.duke.edu/~sf59/Srinivasan_BOE_2014_\n",
      "dataset.htm\n",
      "3OCT-C8 Dataset: http://kaggle.com/datasets/obulisainaren/retinal-oct-c8Table 2\n",
      "Best validation results.\n",
      "Dataset Validation\n",
      "accuracyValidation\n",
      "loss\n",
      "OCT-2017 98.30% 0.0664\n",
      "OCT-2014 99.80% 0.0001\n",
      "OCT-C8 96.99% 0.0472\n",
      "a weight decay of 1ğ‘’âˆ’4. Furthermore, the model was trained for 100\n",
      "epochs with a batch size of 128. The initial learning rate was set to\n",
      "1ğ‘’âˆ’3, with a plateau reduction strategy used across the epochs.\n",
      "4.3. Evaluation methods\n",
      "To assess the classification performance, we utilized the softmax\n",
      "method to transform logits into class probabilities and then selected\n",
      "the highest probability value as the predicted category.\n",
      "ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘§ğ‘–) =exp(ğ‘§ğ‘–)\n",
      "âˆ‘ğ‘›\n",
      "ğ‘—=1exp(ğ‘§ğ‘—)(12)\n",
      "In this study, we utilized accuracy, precision, recall, and F1-score as\n",
      "the evaluation metrics. The following formulas were used to calculate\n",
      "the evaluation metrics.\n",
      "ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =ğ‘‡ğ‘ƒ+ğ‘‡ğ‘\n",
      "ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘(13)\n",
      "ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =ğ‘‡ğ‘ƒ\n",
      "ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ(14)\n",
      "ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ âˆ•ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦ =ğ‘‡ğ‘ƒ\n",
      "ğ‘‡ğ‘ƒ+ğ¹ğ‘(15)\n",
      "ğ¹1_ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = 2 â‹…ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› âˆ—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™\n",
      "ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› +ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™(16)\n",
      "ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦ =ğ‘‡ğ‘\n",
      "ğ‘‡ğ‘+ğ¹ğ‘ƒ(17)\n",
      "Where TP, TN, FP, and FN refer to true positives, true negatives, false\n",
      "positives, and false negatives, respectively. For the four categories of\n",
      "OCT classification, TP was defined as the number of correctly identified\n",
      "cases in a variety, TN as the number of correctly identified negative\n",
      "cases in the modelâ€™s negative class, FP as the number of negative\n",
      "samples that were mistakenly identified as positive classes, and FN\n",
      "as the number of positive cases that were erroneously identified as\n",
      "negative categories.\n",
      "4.4. Results\n",
      "4.4.1. Experimental results\n",
      "During the experiment, we systematically recorded the error and\n",
      "accuracy variations of the training and validation sets, hence that the\n",
      "validation set contains OCT scans that were not used in the training\n",
      "phase. These recorded values were the foundation for evaluating the\n",
      "modelâ€™s classification performance. To visually illustrate the observed\n",
      "changes, we presented in Fig. 6 the error and the accuracy trends of\n",
      "the training set and the validation set of the three datasets which are\n",
      "outlined in 4.1.\n",
      "The curves indicate a rapid decrease in both training and valida-\n",
      "tion errors of each dataset during the initial epochs, followed by a\n",
      "stabilization phase after the 60th epoch. Similarly, the accuracy rate\n",
      "rapidly increases, leveling off after the 60th epoch. Based on the graphs,\n",
      "we observe that the model is doing well since the highest difference\n",
      "between training and validation accuracy is between 0.05% and 0.15%.\n",
      "Therefore, there are no signs of overfitting.\n",
      "Table 2 shows the highest validation accuracies and the lowest\n",
      "validation losses of the proposed HTC-Retina model. Computers in Biology and Medicine 178 (2024) 108726\n",
      "7A. Laouarem et al.\n",
      "Fig. 6. HTC-Retina Performance in the Training and Validation Sets. The blue line presents the accuracy and loss of the training set, while the orange one presents the\n",
      "accuracy and loss of the validation set.\n",
      "Table 3\n",
      "Detailed evaluation results of the HTC-Retina model.\n",
      "Dataset Disease class Evaluation metrics (%)\n",
      "Accuracy Recall/Sensitivity Precision F1-Score Specificity\n",
      "OCT-2017CNV 98.80 99.70 98.36 99.02 99.03\n",
      "DME 99.60 98.76 99.58 99.18 99.86\n",
      "DRUSEN 99.60 99.60 99.60 99.60 99.86\n",
      "NORMAL 99.60 99.60 100.00 99.80 100\n",
      "Average 99.40 99.41 99.39 99.40 99.70\n",
      "OCT-2014AMD 100 100 99.32 99.65 99.66\n",
      "DME 99.32 99.32 100 99.65 100\n",
      "NORMAL 100 100 100 100 100\n",
      "Average 99.77 99.77 99.77 99.76 99.80\n",
      "OCT-C8AMD 100 100 100 100 100\n",
      "CNV 92.85 92.86 95.60 94.20 98.77\n",
      "CSR 100 100 98.87 99.43 99.38\n",
      "DME 92 92 97.28 94.57 99.83\n",
      "DR 99.15 99.14 99.43 99.28 99.63\n",
      "DRUSEN 95.70 95.71 91.78 93.71 99.91\n",
      "MH 99.15 99.14 100 99.57 100\n",
      "NORMAL 97.15 97.14 93.41 95.24 99.02\n",
      "Average 97.00 97.00 97.04 97.01 99.56\n",
      "Fig. 7. Confusion matrix of each dataset generated by the HTC-Retina model .\n",
      "4.4.2. Evaluation results\n",
      "To examine the fine-grained performance of the model, we present a\n",
      "comprehensive classification report on the test datasets in Table 3. This\n",
      "report provides detailed metrics for each disease categoryâ€™s accuracy,\n",
      "recall, precision, F1-score, and specificity.\n",
      "To facilitate a comprehensive analysis of the classification results,\n",
      "we present in Fig. 7 the confusion matrices that were acquired after\n",
      "testing the model on a separate test set.\n",
      "Based on these results, the proposed model demonstrated a notable\n",
      "classification performance in terms of accuracy and sensitivity. More\n",
      "specifically, in the OCT-2014 dataset, the model exhibited remarkableperformance in the classification of AMD, DME, and Normal cases with\n",
      "an accuracy and sensitivity of 99.77%. As we observed in Fig. 7, the\n",
      "proposed model misclassificate one sample from the DME class. In the\n",
      "OCT-C8 dataset, the proposed HRC-Retina model achieves a notable\n",
      "overall accuracy rate of 97.00% for the classification of eight distinct\n",
      "cases. The analysis reveals that the proposed model demonstrates a high\n",
      "level of accuracy in correctly predicting the categories, particularly in\n",
      "cases related to AMD, CSR, DR, and MH, where the predictions are\n",
      "accompanied by a high confidence score. However, there are instances\n",
      "of misclassification observed in the DME vs. CNV cases, as both of\n",
      "these conditions involve the accumulation of fluid, leading to potential Computers in Biology and Medicine 178 (2024) 108726\n",
      "8A. Laouarem et al.\n",
      "Fig. 8. The Grad Class Activation Mapping (Grad-CAM) visualization of the HTC-Retina model. The regions characterized by reddish hues exhibit the highest significance in\n",
      "the classification process, followed by regions containing yellowish pixels, whereas the regions with bluish hues demonstrate the least contribution to the classification outcome.\n",
      "confusion for the HRC-Retina model. Similarly, misclassifications are\n",
      "observed in cases of CNV vs. DRUSEN, which represent the two pri-\n",
      "mary forms of age-related macular degeneration. The complexity and\n",
      "similarities between these conditions may contribute to the modelâ€™s\n",
      "confusion in accurately classifying them. There are also instances of\n",
      "misclassification observed in the normal and other classes, particu-\n",
      "larly when dealing with cases that exhibit subtle lesions. Identifying\n",
      "such cases poses a challenge even for experienced medical experts.\n",
      "Regarding the OCT-2017 dataset, the model successfully classified 980\n",
      "out of 986 cases, resulting in an accuracy and sensitivity of 99.40%.\n",
      "This achievement highlights the modelâ€™s remarkable predictive perfor-\n",
      "mance. However, a few misclassifications were observed in cases of\n",
      "CNV and DME, due to the previously mentioned reason.\n",
      "4.4.3. Features map visualization\n",
      "In medical imaging analysis, the explainability of deep learning\n",
      "models is important due to ethics and life health involvement. Indeed,\n",
      "it brings transparency, interpretability, and validation to the deep\n",
      "learning models, empowering clinicians, researchers, and patients to\n",
      "understand, trust, and effectively utilize artificial intelligence technolo-\n",
      "gies in healthcare. In this section, we present the Class Activation\n",
      "Mapping (CAM) visualizations using the Grad-CAM method [ 46] as\n",
      "a way to visualize the evidence supporting the modelâ€™s predictions.\n",
      "The CAM technique generates a heat map highlighting regions within\n",
      "the image strongly associated with the target class. Intensified red\n",
      "coloration indicates a higher correlation with the predicted category.\n",
      "In Fig. 8, the Grad-CAM visualization distinctly delineates the regions\n",
      "of interest. Notably, in the OCT images of diseased conditions, the\n",
      "lesion regions exhibit intensified red coloration, indicating the modelâ€™s\n",
      "attention to the areas that are diagnostically significant and aligning\n",
      "with the focus of ophthalmologists during decision-making processes.\n",
      "4.4.4. Validation and comparative analysis\n",
      "To validate the effectiveness of the HTC-Retina model, we com-\n",
      "pared its performance with widely used classification approaches:\n",
      "VGG19 [ 17] and ResNet50 [ 22], pure ViT, and other SOTA meth-\n",
      "ods. Table 4 presents a comparative analysis of different modelsâ€™\n",
      "performances on the three used datasets for OCT image classification.\n",
      "In a comparative analysis of various models, the HTC-Retina con-\n",
      "sistently demonstrated superior performance, surpassing SOT methods\n",
      "and other selected architectures across all three datasets. As depicted\n",
      "in Table 4, HTC-Retina achieved remarkable accuracy percentages of\n",
      "99.40%, 99.77%, and 97.00%, along with sensitivities of 99.41%,99.77 and 97.00% on the OCT-2017, OCT-2014, and OCT-C8 datasets.\n",
      "These results surpassed CNN-based models and baseline approaches\n",
      "such as VGG-19, ResNet50, InceptionV3, ViT, and hybrid-based CNN-\n",
      "ViT models like ConViT, CoAtNet, LLCT, and HCTNet. Notably, in the\n",
      "OCT-2017 dataset, HTC-Retina outperformed ConViT, CoAtNet, LLCT,\n",
      "and HCTNet by 4.94, 1.58, 1.7, and 7.84 percentage points (pp.) in\n",
      "accuracy, respectively. In the OCT-2014 dataset, HTC-Retina surpassed\n",
      "SoT, CoAtNet, and HCTNet by 0.42, 0.92, and 13.59 pp., respectively.\n",
      "These experimental results show that the proposed HTC-Retina can\n",
      "achieve the desired classification performance on the OCT-2017, OCT-\n",
      "2014, and OCT-C8 datasets, further validating the rationality of our\n",
      "model design.\n",
      "The improvement brought about by HTC-Retina in comparison to a\n",
      "specific method is quantified by the ratio of the metric value of HTC-\n",
      "Retina to the corresponding value of that method. This improvement is\n",
      "denoted as â€˜â€˜gainâ€™â€™, as all the evaluation metrics aim for maximization.\n",
      "Using Table 4, we promptly compute this gain, for each metric. For\n",
      "instance, if the metric is accuracy and the SOTA method is ConViT, then\n",
      "the HTC-Retina accuracy gain with respect to ConViT is computed as\n",
      "99.40/94.46 = 1.0522. Note that any gain must exceed unity strictly;\n",
      "otherwise, it is regarded as a loss. This calculation is performed for\n",
      "overall accuracy, sensitivity, specificity and all SOTA methods across\n",
      "all datasets. These gain values are visualized in charts in Fig. 9. The\n",
      "findings are evident: All gains are higher than unity and therefore,\n",
      "HTC-Retina exhibits no loss compared to any of the SOTA methods,\n",
      "across all metrics and all datasets; one exception being the specificity of\n",
      "Pure ViT for OCT-2014 dataset, being 99.87 compared to our method,\n",
      "yielding 99.80. However, even in this exceptional adversarial case, the\n",
      "gain remains very close to 1 (exactly 0.999).\n",
      "4.4.5. Ablation study\n",
      "Four ablation studies were conducted to validate the effectiveness\n",
      "of the proposed HTC-Retina model. These studies aimed to assess the\n",
      "impact of the CPTE and RDP-ConvNet modules on the performance\n",
      "of HTC-Retina in OCT scans. Furthermore, the influence of removing\n",
      "positional embeddings from the model was also examined. The effects\n",
      "of each modification on the proposed HTC-Retina model are presented\n",
      "in Table 5.\n",
      "The results presented in Table 5 provide valuable insights into the\n",
      "effects of these modifications on the performance of HTC-Retina across\n",
      "different datasets.\n",
      "In the first two ablation studies, the HTC-Retina architecture was\n",
      "evaluated by excluding the RDP-ConvNet module while retaining the Computers in Biology and Medicine 178 (2024) 108726\n",
      "9A. Laouarem et al.\n",
      "Table 4\n",
      "Comparison of classification results of different models on the test set.\n",
      "Datasets Models Evaluation matrices (%)\n",
      "Accuracy Recall/Sensitivity Precision F1-Score Specificity\n",
      "OCT-2017VGG-19 94.01 94.01 94.01 94.01 96.08\n",
      "ResNet50 94.83 94.63 95.12 94.87 96.35\n",
      "Pure ViT 93.90 93.85 94.05 93.95 95.20\n",
      "InceptionV3 [ 19] 96.6 97.80 â€“ â€“ 97.40\n",
      "Deep CNN [ 24] 98.65 98.50 98.75 98.50 â€“\n",
      "Multi-ResNet50 [ 21] 90.2 86.9 86.5 â€“ 96.60\n",
      "ConViT (Hybrid) [ 34] 94.46 94.00 94.00 94.00 â€“\n",
      "CoAtNet (Hybrid) [ 31] 97.82 97.82 97.86 97.84 99.38\n",
      "LLCT (Hybrid) [ 35] 97.70 97.70 97.80 97.75 99.20\n",
      "HCTNet (Hybrid) [ 33] 91.56 88.57 88.11 â€“ â€“\n",
      "HTC-Retina (Ours) 99.40 99.41 99.39 99.40 99.70\n",
      "OCT-2014VGG-19 99.32 99.32 99.62 99.47 99.75\n",
      "ResNet50 99.20 99.15 99.50 99.32 99.66\n",
      "Pure ViT 99.52 99.45 99.64 99.55 99.87\n",
      "CNN [ 15] 92.64 90.21 â€“ â€“ 95.88\n",
      "Multi-ResNet50 [ 21] 95.53 â€“ â€“ â€“ â€“\n",
      "SoT (Hybrid Model) [ 32] 99.35 99.25 99.25 99.25 99.55\n",
      "CoAtNet (Hybrid) [ 31] 98.85 98.83 98.80 98.81 99.38\n",
      "HCTNet (Hybrid) [ 33] 86.18 85.40 88.53 â€“ â€“\n",
      "HTC-Retina (Ours) 99.77 99.77 99.77 99.76 99.80\n",
      "OCT-C8VGG-19 90 89.98 90.03 90 94.32\n",
      "ResNet50 91.50 91.43 92.12 91.77 94.79\n",
      "Pure ViT 92.65 92.52 93.03 92.77 94.98\n",
      "ResNet34 [ 20] 92.40 92.00 92.00 92.00 â€“\n",
      "ResNet50 [ 20] 90.30 91.00 91.00 91.00 â€“\n",
      "ResNet101 [ 20] 84.50 86.00 86.00 86.00 â€“\n",
      "CoAtNet (Hybrid) [ 31] 94.92 94.92 95.02 94.93 97.26\n",
      "HTC-Retina (Ours) 97.00 97.00 97.04 97.01 99.56\n",
      "Fig. 9. HTC-Retina Gains. Computers in Biology and Medicine 178 (2024) 108726\n",
      "10A. Laouarem et al.\n",
      "Table 5\n",
      "Ablative study on CPTE, RDP-ConvNet, and Positional embeddings.\n",
      "Model CPTE PE ViTE RDP-ConvNet Accuracy (%)\n",
      "OCT-2017 OCT-2014 OCT-C8\n",
      "Val. Test. Val. Test. Val. Test.\n",
      "HTC-RetinaNo Yes Yes No 92.75 93.90 99.49 99.52 92.58 92.65\n",
      "Yes Yes Yes No 95.10 98.14 99.60 99.62 94.36 94.35\n",
      "Yes Yes Yes Yes 97.92 99.03 99.71 99.72 96.65 96.70\n",
      "Yes No Yes Yes 98.30 99.40 99.80 99.77 96.99 97.00\n",
      "Table 6\n",
      "Comparison results on the noisy and original test set of each dataset.\n",
      "Datasets Type Evaluation matrices (%)\n",
      "Accuracy Recall/Sensitivity Precision F1-Score Specificity\n",
      "OCT-2017Original 99.40 99.41 99.39 99.40 99.70\n",
      "Noisy 99.07 99.08 99.06 99.07 99.62\n",
      "OCT-2014Original 99.77 99.77 99.77 99.76 99.80\n",
      "Noisy 98.13 98.13 98.22 98.17 99.60\n",
      "OCT-C8Original 97.00 97.00 97.04 97.01 99.56\n",
      "Noisy 94.70 94.71 94.79 94.70 99.23\n",
      "positional embeddings (PE) and ViTE modules. These studies aimed\n",
      "to examine the influence of the CPTE module on the overall perfor-\n",
      "mance of the HTC-Retina model. The results indicated a decline in\n",
      "accuracy across the validation and test sets. The test accuracy of the\n",
      "model exhibited notable gains, with improvements of 4.24% on the\n",
      "OCT-2017 dataset, 0.13% on OCT-2014, and 1.7% on OCT-C8. These\n",
      "findings indicate that the CPTE module holds significant importance in\n",
      "enhancing the modelâ€™s performance by effectively capturing pertinent\n",
      "local features, mainly when dealing with small datasets.\n",
      "The third ablation study was conducted to explore the influence of\n",
      "the RDP-ConvNet module on the HTC-Retina architecture. The experi-\n",
      "mental results consistently improved the modelâ€™s accuracy across all the\n",
      "validation and test sets. The employment of the depthwise and point-\n",
      "wise convolution operations boosted the modelâ€™s performance, resulting\n",
      "in test accuracy gains of 0.89% on OCT-2017, 0.1% on OCT-2014,\n",
      "and 2.35% on OCT-C8. This finding underscores the significance of the\n",
      "RDP-ConvNet module in effectively capturing fine-grained details and\n",
      "high-level local features, thereby enhancing the modelâ€™s discriminative\n",
      "performance.\n",
      "To validate the study as reported in [10] and further investigate\n",
      "the incorporation of enhanced local information within the HTC-Retina\n",
      "model, a final ablation study was conducted to assess the necessity\n",
      "of position embedding. The findings indicate that removing position\n",
      "embeddings does not adversely impact the performance of the proposed\n",
      "model. On the contrary, their omission leads to an improvement in\n",
      "accuracy, with enhancements of 0.37% observed in the OCT-2017\n",
      "dataset, 0.05% in OCT-2014, and 0.30% in the OCT-C8 dataset, as\n",
      "reported in [10]. Hence, the final architecture of the HTC-Retina model\n",
      "adopts the exclusion of position embedding.\n",
      "4.4.6. Noise robustness evaluation\n",
      "To assess the robustness of the HTC-Retina model to Gaussian\n",
      "noise, we introduced noise to the test set of each dataset. The average\n",
      "Peak Signal-to-Noise Ratio (PSNR) between the original and the noisy\n",
      "versions was 28.4, indicating a moderate noise level. We also computed\n",
      "the Structural Similarity Index (SSIM) further to assess the similarity\n",
      "between the original and noisy images, obtaining a value of 57.9.\n",
      "Without retraining the model on the noisy datasets, we directly\n",
      "evaluated its performance using the pre-trained model on the origi-\n",
      "nal datasets. The comprehensive evaluation metrics are presented in\n",
      "Table 6, illustrating the comparison between the modelâ€™s performance\n",
      "on the original and noisy datasets. The results indicate that the HTC-\n",
      "Retina model exhibits commendable performance in the presence of\n",
      "noise, showcasing its robustness compared to its performance on the\n",
      "original dataset.4.4.7. External validation\n",
      "To evaluate the performance and generalizability of the HTC-Retina\n",
      "model, an external validation was conducted, encompassing the assess-\n",
      "ment of the model on independent datasets to ascertain its reliability,\n",
      "robustness, and suitability. Specifically, the pre-trained HTC-Retina\n",
      "model, originally trained on the OCT-C8 dataset, was evaluated on\n",
      "additional datasets. The performance of the pre-trained model on the\n",
      "OCT-2014 and OCT-2017 datasets is presented in Table 7.\n",
      "Based on the aforementioned results, the performance of the HTC-\n",
      "Retina model that pre-trained on OCT-C8 in disease prediction was\n",
      "notable when applied to two distinct OCT datasets: OCT-2017, com-\n",
      "prising cases of CNV, DME, DRUSEN, and Normal, and OCT-2014,\n",
      "consisting of cases of AMD, DME, and Normal. Therefore, it is evi-\n",
      "dent that the model demonstrates accurate classification capabilities,\n",
      "yielding reliable predictions for OCT scan classification tasks. This\n",
      "evaluation proves that our model exhibits independence from specific\n",
      "data samples, demonstrating its robustness and generalizability to di-\n",
      "verse datasets. The results indicate the modelâ€™s capacity to effectively\n",
      "accommodate variations in image characteristics, acquisition settings,\n",
      "and potential temporal changes between different datasets.\n",
      "4.5. Discussion\n",
      "The main objective of our study was to develop an innovative\n",
      "and precise hybrid model, named HTC-Retina, for the classification\n",
      "and identification of retinal diseases using OCT scans. Inspired from\n",
      "many recent research [8,31,33,41], the proposed HTC-Retina model\n",
      "was constructed by integrating ViT and CNN architectures. This syn-\n",
      "ergistic approach effectively addressed the limitations inherent in each\n",
      "architecture and capitalized on their respective strengths, resulting in\n",
      "a highly proficient model for the classification task of OCT scans. In\n",
      "contrast to SOTA hybrid models [32,33,35], which incorporate CNN\n",
      "blocks preceding the ViT architecture, our proposed HTC-Retina model\n",
      "directly utilizes the output sequence of tokens of the CPTE module\n",
      "as input to the ViT encoder, eliminating patch splitting, projection,\n",
      "and positional embedding. This novel approach enhances model flex-\n",
      "ibility, accommodating input resolutions beyond strict divisibility by\n",
      "the pre-set patch size. The CPTE module efficiently embeds the image,\n",
      "generating more nuanced tokens for the transformer. Additionally, the\n",
      "CPTE module is a straightforward CNN network, characterized by its\n",
      "simplicity and reduced complexity, in contrast to the module employed\n",
      "in [33,34], which integrated dense blocks and residual blocks. Regard-\n",
      "ing the ViT component of our proposed model, the final configuration\n",
      "comprises merely four encoder blocks, in contrast to HCTNet [33]\n",
      "and LLCT [35], both employing 12 blocks. This design choice ren-\n",
      "ders our model less complex. As for the remaining components the Computers in Biology and Medicine 178 (2024) 108726\n",
      "11A. Laouarem et al.\n",
      "Table 7\n",
      "External validation results of the HTC-Retina model.\n",
      "Model Datasets Evaluation matrices (%)\n",
      "Accuracy Recall/Sensitivity Precision F1-Score Specificity\n",
      "HTC-Retina Pre-trained on OCT-C8OCT-2017 95.23 95.30 95.23 95.23 95.22\n",
      "OCT-2014 94.37 94.45 94.37 94.31 96.88\n",
      "proposed model employs the same structure of the HCTNet but using\n",
      "different techniques. While HCTNet incorporates a parallel convolu-\n",
      "tional branch with residual dense blocks and a fully connected layer,\n",
      "HCT-Retina adopts a lightweight architecture, based on depthwise\n",
      "and pointwise convolution operation (RDP-ConvNet module), akin to\n",
      "ViT. Operating directly on patches as input, RDP-ConvNet aims to\n",
      "capture relevant local features through a less expressive operation. For\n",
      "the combination module, HTC-Retina utilizes global average pooling\n",
      "which reduces dimensionality, enhances translation invariance, im-\n",
      "proves generalization, and reduces sensitivity instead of a re-weighting\n",
      "mechanism. The network further simplifies and reduces complexity by\n",
      "directly concatenating output features, facilitating access to multi-scale\n",
      "features.\n",
      "To evaluate the effectiveness of the proposed HTC-Retina, a compar-\n",
      "ative analysis was conducted across three distinct datasets, benchmark-\n",
      "ing against SOTA methods, including hybrid ViT-CNN models, CNN\n",
      "models, and baseline approaches. The results, as detailed in Section 4.4,\n",
      "underscore the efficacy and feasibility of HTC-Retina in classifying OCT\n",
      "diseases, surpassing other SOTA methods. Additionally, to validate its\n",
      "remarkable performance, the model underwent testing on noisy ver-\n",
      "sions of the datasets and external validation. The notable performance\n",
      "in these scenarios underscores the robustness and generalizability of\n",
      "HTC-Retina across diverse datasets.\n",
      "Despite the successes demonstrated by our proposed hybrid model,\n",
      "HTC-Retina, it is essential to acknowledge certain limitations inherent\n",
      "in our study. The primary constraint lies in the lack of diversity and\n",
      "abundance in real sample datasets, necessitating the integration of\n",
      "augmented data to compensate. This potential source of bias may\n",
      "influence the modelâ€™s robustness in real-world scenarios. Additionally,\n",
      "the modelâ€™s performance, particularly in diseases with shared char-\n",
      "acteristics (CNV, DME, DRUSEN) or diseases with tiny lesions that\n",
      "require very great scrutiny, even for an expert, encounters difficulties,\n",
      "particularly in the presence of numerous other diseases. The fine-tuning\n",
      "process for the hybrid architecture poses another challenge, demand-\n",
      "ing meticulous attention to hyperparameter optimization. Achieving\n",
      "an optimal balance between transformer and CNN components may\n",
      "necessitate extensive experimentation, impacting the scalability of the\n",
      "model.\n",
      "5. Conclusion\n",
      "Retinal diseases are significant contributors to temporary or per-\n",
      "manent visual impairments. This study introduces a hybrid model\n",
      "that combines a transformer and a CNN architecture, specifically de-\n",
      "signed for Retinal OCT image classification tasks on small and medium\n",
      "datasets. The goal is to accurately identify the type of retinal disease\n",
      "present. Through a series of experiments, we investigated the effec-\n",
      "tiveness of the CPTE and RDP-ConvNet modules. These modules were\n",
      "found to be critical factors contributing to the improved performance\n",
      "of the HTC-Retina model. The performance of the HTC-Retina model\n",
      "was evaluated on the OCT-2017, OCT-C8, and OCT-2014 datasets. It\n",
      "outperformed previous works and established models, achieving accu-\n",
      "racy rates of 99.40%, 97.00%, and 99.77% respectively, and sensitivity\n",
      "rates of 99.41%, 97.00%, and 99.77% respectively. Notably, the model\n",
      "demonstrated high performance while maintaining computational effi-\n",
      "ciency. Importantly, the proposed model no longer relies on position\n",
      "embedding, as the combination of CPTE and RDP-ConvNet with theViT enables the capture of more local features. This advancement\n",
      "contributes to the modelâ€™s ability to accurately classify retinal diseases.\n",
      "Furthermore, external validation and robustness evaluations were\n",
      "conducted to assess the modelâ€™s generalization ability and performance\n",
      "under different conditions. The HTC-Retina model exhibited remark-\n",
      "able performance on new data, demonstrating its ability to generalize\n",
      "well beyond the training datasets. Additionally, the model demon-\n",
      "strated robustness when subjected to external noise, further enhancing\n",
      "its practical applicability.\n",
      "While our model demonstrated high classification performance on\n",
      "the evaluated datasets, its applicability to diverse datasets and diseases\n",
      "necessitates further investigation. Additionally, exploring alternative\n",
      "configurations and identifying optimal parameters is imperative to\n",
      "address the mentioned model limitations. In future research, we plan\n",
      "to apply the proposed model to additional real-world OCT image\n",
      "datasets and further investigate its potential for diverse medical im-\n",
      "ages and diseases. This approach aims to enhance our understand-\n",
      "ing of the modelâ€™s generalization performance, validate its effective-\n",
      "ness in a broader range of clinical scenarios, and address its limita-\n",
      "tions by exploring alternative configurations and identifying optimal\n",
      "parameters.\n",
      "CRediT authorship contribution statement\n",
      "Ayoub Laouarem: Writing â€“ review & editing, Writing â€“\n",
      "original draft, Visualization, Software, Methodology, Investigation,\n",
      "Formal analysis, Conceptualization. Chafia Kara-Mohamed: Writing\n",
      "â€“ review & editing, Validation, Supervision, Project administration,\n",
      "Conceptualization. El-Bay Bourennane: Writing â€“ review & editing,\n",
      "Validation, Supervision. Aboubekeur Hamdi-Cherif: Writing â€“ review\n",
      "& editing, Validation.\n",
      "Declaration of competing interest\n",
      "The authors declare that they have no known competing financial\n",
      "interests or personal relationships that could have appeared to influence\n",
      "the work reported in this paper.\n",
      "References\n",
      "[1] J. Zhao, Y. Lu, Y. Qian, Y. Luo, W. Yang, Emerging trends and research foci in\n",
      "artificial intelligence for retinal diseases: Bibliometric and visualization study, J.\n",
      "Med. Internet Res. 24 (6) (2022) e37532, http://dx.doi.org/10.2196/37532.\n",
      "[2] L.S. Lim, P. Mitchell, J.M. Seddon, F.G. Holz, T.Y. Wong, Age-related macular\n",
      "degeneration, Lancet 379 (2012) 1728â€“1738, http://dx.doi.org/10.1016/S0140-\n",
      "6736(12)60282-7.\n",
      "[3] G.E. Lang, Diabetic macular edema, Ophthalmologica 227 (Suppl. 1) (2012)\n",
      "21â€“29, http://dx.doi.org/10.1159/000337156.\n",
      "[4] A. Abdelsalam, D.P. Lucian, M.A. Zarbin, Drusen in age-related macular de-\n",
      "generation: Pathogenesis, natural course, and laser photocoagulationâ€“Induced\n",
      "regression, Surv. Ophthalmol. 44 (1) (1999) 1â€“29, http://dx.doi.org/10.1016/\n",
      "S0039-6257(99)00072-7.\n",
      "[5] W.-M. Chan, M. Ohji, T.Y.Y. Lai, D.T.L. Liu, Y. Tano, D.S.C. Lam, Choroidal\n",
      "neovascularisation in pathological myopia: An update in management, Br. J.\n",
      "Ophthalmol. 89 (11) (2005) 1522â€“1528, http://dx.doi.org/10.1136/bjo.2005.\n",
      "074716.\n",
      "[6] A.C. Weenink, R.A. Borsje, J.A. Oosterhuis, Familial chronic central serous\n",
      "chorioretinopathy, Ophthalmologica 215 (3) (2001) 183â€“187, http://dx.doi.org/\n",
      "10.1159/000050855. Computers in Biology and Medicine 178 (2024) 108726\n",
      "12A. Laouarem et al.\n",
      "[7] D. Huang, E.A. Swanson, C.P. Lin, J.S. Schuman, W.G. Stinson, W. Chang,\n",
      "M.R. Hee, T. Flotte, K. Gregory, C.A. Puliafito, J.G. Fujimoto, Optical coherence\n",
      "tomography, Science 254 (5035) (1991) 1178â€“1181, http://dx.doi.org/10.1126/\n",
      "science.1957169.\n",
      "[8] A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, H. Shi, Escaping the big\n",
      "data paradigm with compact transformers, 2022, arXiv:2104.05704.\n",
      "[9] Z. Dai, H. Liu, Q.V. Le, M. Tan, CoAtNet: Marrying convolution and attention\n",
      "for all data sizes, 2021, arXiv:2106.04803.\n",
      "[10] S. dâ€™Ascoli, H. Touvron, M.L. Leavitt, A.S. Morcos, G. Biroli, L. Sagun, ConViT:\n",
      "Improving vision transformers with soft convolutional inductive biases, J. Stat.\n",
      "Mech. Theory Exp. 2022 (11) (2022) 114005, http://dx.doi.org/10.1088/1742-\n",
      "5468/ac9830.\n",
      "[11] B. Sun, J. Li, M. Shao, Y. Fu, LPRNet: Lightweight deep network by low-rank\n",
      "pointwise residual convolution, 2019, arXiv:1910.11853.\n",
      "[12] B. Sun, J. Li, M. Shao, Y. Fu, LRPRNet: Lightweight deep network by low-rank\n",
      "pointwise residual convolution, IEEE Trans. Neural Netw. Learn. Syst. 34 (8)\n",
      "(2023) 4440â€“4450, http://dx.doi.org/10.1109/TNNLS.2021.3117685.\n",
      "[13] Y. Rong, D. Xiang, W. Zhu, K. Yu, F. Shi, Z. Fan, X. Chen, Surrogate-assisted\n",
      "retinal OCT image classification based on convolutional neural networks, IEEE\n",
      "J. Biomed. Health Inf. 23 (1) (2019) 253â€“263, http://dx.doi.org/10.1109/JBHI.\n",
      "2018.2795545.\n",
      "[14] M. Treder, J. Lauermann, N. Eter, Utomated detection of exudative age-related\n",
      "macular degeneration in spectral domain optical coherence tomography using\n",
      "deep learning, Graefeâ€™s Arch. Clin. Exp. Ophthalmol. 256 (2018) 259â€“265,\n",
      "http://dx.doi.org/10.1007/s00417-017-3850-3.\n",
      "[15] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the inception\n",
      "architecture for computer vision, in: 2016 IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, CVPR, 2016, pp. 2818â€“2826, http://dx.doi.org/10.\n",
      "1109/CVPR.2016.308.\n",
      "[16] X. Li, L. Shen, M. Shen, F. Tan, C.S. Qiu, Deep learning based early stage diabetic\n",
      "retinopathy detection using optical coherence tomography, Neurocomputing 369\n",
      "(2019) 134â€“144, http://dx.doi.org/10.1016/j.neucom.2019.08.079.\n",
      "[17] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale\n",
      "image recognition, in: International Conference on Learning Representations,\n",
      "2015.\n",
      "[18] S. He, J. Zheng, A. Maehara, G. Mintz, D. Tang, M. Anastasio, H. Li, Convolu-\n",
      "tional neural network based automatic plaque characterization for intracoronary\n",
      "optical coherence tomography images, in: Medical Imaging, 2018, http://dx.doi.\n",
      "org/10.1117/12.2293957.\n",
      "[19] D.S. Kermany, et al., Identifying medical diagnoses and treatable diseases by\n",
      "image-based deep learning, Cell 172 (2018) 1122â€“1131.e9, http://dx.doi.org/\n",
      "10.1016/j.cell.2018.02.010.\n",
      "[20] K. Karthik, M. Mahadevappa, Convolution neural networks for optical coherence\n",
      "tomography (OCT) image classification, Biomed. Signal Process. Control 79\n",
      "(2023) 104176, http://dx.doi.org/10.1016/j.bspc.2022.104176.\n",
      "[21] F. Li, H. Chen, Z. Liu, X. dian Zhang, M. shan Jiang, Z. zheng Wu, K. qian\n",
      "Zhou, Deep learning-based automated detection of retinal diseases using optical\n",
      "coherence tomography images, Biomed. Opt. Express 10 (12) (2019) 6204â€“6226,\n",
      "http://dx.doi.org/10.1364/BOE.10.006204.\n",
      "[22] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,\n",
      "in: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR,\n",
      "2016, pp. 770â€“778, http://dx.doi.org/10.1109/CVPR.2016.90.\n",
      "[23] G. Altan, DeepOCT: An explainable deep learning architecture to analyze macular\n",
      "edema on OCT images, Eng. Sci. Technol., Int. J. 34 (2022) 101091, http:\n",
      "//dx.doi.org/10.1016/j.jestch.2021.101091.\n",
      "[24] M. Berrimi, A. Moussaoui, Deep learning for identifying and classifying retinal\n",
      "diseases, in: 2020 2nd International Conference on Computer and Information\n",
      "Sciences, ICCIS, 2020, pp. 1â€“6, http://dx.doi.org/10.1109/ICCIS49240.2020.\n",
      "9257674.\n",
      "[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, Å. Kaiser,\n",
      "I. Polosukhin, Attention is all you need, in: Advances in Neural Information\n",
      "Processing Systems, vol. 30, 2017.[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,\n",
      "M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, An\n",
      "image is worth 16x16 words: Transformers for image recognition at scale, 2021,\n",
      "arXiv:2010.11929.\n",
      "[27] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin transformer:\n",
      "Hierarchical vision transformer using shifted windows, in: 2021 IEEE/CVF\n",
      "International Conference on Computer Vision, ICCV, 2021, pp. 9992â€“10002,\n",
      "http://dx.doi.org/10.1109/ICCV48922.2021.00986.\n",
      "[28] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, C. Shen, Twins:\n",
      "Revisiting the design of spatial attention in vision transformers, 2021, arXiv:\n",
      "2104.13840.\n",
      "[29] C.-F.R. Chen, Q. Fan, R. Panda, CrossViT: Cross-attention multi-scale vision\n",
      "transformer for image classification, in: 2021 IEEE/CVF International Conference\n",
      "on Computer Vision, ICCV, 2021, pp. 347â€“356, http://dx.doi.org/10.1109/\n",
      "ICCV48922.2021.00041.\n",
      "[30] X. Xie, J. Niu, X. Liu, Z. Chen, S. Tang, S. Yu, A survey on incorporating domain\n",
      "knowledge into deep learning for medical image analysis, Med. Image Anal. 69\n",
      "(2021) 101985, http://dx.doi.org/10.1016/j.media.2021.101985.\n",
      "[31] Z. Dai, H. Liu, Q.V. Le, M. Tan, CoAtNet: Marrying convolution and attention\n",
      "for all data sizes, 5, 2021, pp. 3965â€“3977,\n",
      "[32] J. Shen, Y. Hu, X. Zhang, Y. Gong, R. Kawasaki, J. Liu, Structure-oriented\n",
      "transformer for retinal diseases grading from OCT images, Comput. Biol. Med.\n",
      "152 (2023) 106445, http://dx.doi.org/10.1016/j.compbiomed.2022.106445.\n",
      "[33] Z. Ma, Q. Xie, P. Xie, F. Fan, X. Gao, J. Zhu, HCTNet: A hybrid\n",
      "ConvNet-transformer network for retinal optical coherence tomography image\n",
      "classification, Biosensors 12 (7) (2022) http://dx.doi.org/10.3390/bios12070542.\n",
      "[34] P. Dutta, K.A. Sathi, M.A. Hossain, M.A.A. Dewan, Conv-ViT: A convolution and\n",
      "vision transformer-based hybrid feature extraction method for retinal disease\n",
      "detection, J. Imaging 9 (7) (2023) http://dx.doi.org/10.3390/jimaging9070140.\n",
      "[35] H. Wen, J. Zhao, S. Xiang, L. Lin, C. Liu, T. Wang, L. An, L. Liang, B. Huang,\n",
      "Towards more efficient ophthalmic disease classification and lesion location\n",
      "via convolution transformer, Comput. Methods Programs Biomed. 220 (2022)\n",
      "http://dx.doi.org/10.1016/j.cmpb.2022.106832.\n",
      "[36] A. Abien Fred, Deep learning using rectified linear units (ReLU), 2019, arXiv:\n",
      "1803.08375.\n",
      "[37] J.L. Ba, J.R. Kiros, G.E. Hinton, Layer normalization, 2016, arXiv:1607.06450.\n",
      "[38] G. Huang, Y. Sun, Z. Liu, D. Sedra, K.Q. Weinberger, Deep networks with\n",
      "stochastic depth, in: Computer Vision â€“ ECCV 2016, 2016, pp. 646â€“661.\n",
      "[39] D. Hendrycks, K. Gimpel, Gaussian error linear units (GELUs), 2023, arXiv:\n",
      "1606.08415.\n",
      "[40] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,\n",
      "in: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR,\n",
      "2016, pp. 770â€“778, http://dx.doi.org/10.1109/CVPR.2016.90.\n",
      "[41] A. Trockman, J.Z. Kolter, Patches are all you need?, 2022, arXiv:2201.09792.\n",
      "[42] Z. Zhang, M.R. Sabuncu, Generalized cross entropy loss for training deep neural\n",
      "networks with noisy labels, in: Proceedings of the 32nd International Conference\n",
      "on Neural Information Processing Systems, 2018, pp. 8792â€“8802.\n",
      "[43] I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016, http:\n",
      "//www.deeplearningbook.org.\n",
      "[44] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S.\n",
      "Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore,\n",
      "D.G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, X.\n",
      "Zheng, TensorFlow: A system for large-scale machine learning, in: Proceedings of\n",
      "the 12th USENIX Conference on Operating Systems Design and Implementation,\n",
      "2016, pp. 265â€“283.\n",
      "[45] I. Loshchilov, F. Hutter, Decoupled weight decay regularization, 2019, arXiv:\n",
      "1711.05101.\n",
      "[46] R.R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-\n",
      "CAM: Visual explanations from deep networks via gradient-based localization,\n",
      "in: 2017 IEEE International Conference on Computer Vision, ICCV, 2017, pp.\n",
      "618â€“626, http://dx.doi.org/10.1109/ICCV.2017.74.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m myAudio \u001b[38;5;241m=\u001b[39m gTTS(text\u001b[38;5;241m=\u001b[39mtextString, lang\u001b[38;5;241m=\u001b[39mlanguage, slow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Save as mp3 file\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mmyAudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAudio.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gtts\\tts.py:335\u001b[0m, in \u001b[0;36mgTTS.save\u001b[1;34m(self, savefile)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do the TTS API request and write result to file.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(savefile), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_to_fp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m     f\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    337\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, savefile)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gtts\\tts.py:316\u001b[0m, in \u001b[0;36mgTTS.write_to_fp\u001b[1;34m(self, fp)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do the TTS API request(s) and write bytes to a file-like object.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpart-\u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[38;5;124;43m written to \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gtts\\tts.py:268\u001b[0m, in \u001b[0;36mgTTS.stream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;66;03m# Send request\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetproxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders-\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, idx, r\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m    276\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl-\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, idx, r\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32md:\\Python\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Python\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Python\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Python\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "# Importing Google Text to Speech library\n",
    "from gtts import gTTS\n",
    "\n",
    "# Importing PDF reader PyPDF2\n",
    "import PyPDF2\n",
    "\n",
    "# Open file Path\n",
    "pdf_File = open('Vit-retina.pdf', 'rb') \n",
    "\n",
    "# Create PDF Reader Object\n",
    "pdf_Reader = PyPDF2.PdfReader(pdf_File)\n",
    "count = len(pdf_Reader.pages)  # Updated way to count number of pages\n",
    "textList = []\n",
    "\n",
    "# Extracting text data from each page of the pdf file\n",
    "for i in range(count):\n",
    "    try:\n",
    "        page = pdf_Reader.pages[i]  # Access pages using the updated method\n",
    "        textList.append(page.extract_text())  # Use extract_text method\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from page {i}: {e}\")\n",
    "\n",
    "# Converting multiline text to single line text\n",
    "textString = \" \".join(textList)\n",
    "\n",
    "print(textString)\n",
    "\n",
    "# Set language to English (en)\n",
    "language = 'en'\n",
    "\n",
    "# Call GTTS\n",
    "myAudio = gTTS(text=textString, lang=language, slow=False)\n",
    "\n",
    "# Save as mp3 file\n",
    "myAudio.save(\"Audio.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gTTs\n",
      "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\benar\\appdata\\roaming\\python\\python312\\site-packages (from gTTs) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\benar\\appdata\\roaming\\python\\python312\\site-packages (from gTTs) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\benar\\appdata\\roaming\\python\\python312\\site-packages (from click<8.2,>=7.1->gTTs) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benar\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->gTTs) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benar\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->gTTs) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benar\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->gTTs) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benar\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->gTTs) (2024.8.30)\n",
      "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gTTs\n",
      "Successfully installed gTTs-2.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gTTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
